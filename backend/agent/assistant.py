# backend/agent/assistant.py
import asyncio
import json
import aiohttp
import logging
from livekit import api, rtc
from livekit.agents import llm
from livekit.agents.llm import ChatContext, FunctionCall, FunctionCallOutput
from livekit.agents.stt import SpeechEventType

from core.config import settings
from core.exceptions import LiveKitConnectionError
from integrations.aliyun.stt import AliyunSTT
from integrations.aliyun.llm import create_llm
from integrations.aliyun.tts import create_tts
from integrations.tools.manager import tool_manager

logger = logging.getLogger(__name__)


class AIAssistant:
    """AI è¯­éŸ³åŠ©æ‰‹"""

    def __init__(self):
        self.room = None
        self.audio_source = None
        self.http_session = None
        self.stt = None
        self.llm = None
        self.tts = None
        self.tool_manager = tool_manager

    async def initialize(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        logger.info("åˆå§‹åŒ– AI ç»„ä»¶...")

        self.http_session = aiohttp.ClientSession()

        self.stt = AliyunSTT(
            api_key=settings.DASHSCOPE_API_KEY,
            model='paraformer-realtime-v2',
            language='zh-CN'
        )

        self.llm = create_llm()
        self.tts = create_tts(self.http_session)

        logger.info("âœ… AI ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    async def _execute_tool(self, function_name: str, arguments: dict) -> str:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        try:
            logger.info(f"ğŸ”§ æ‰§è¡Œå·¥å…·: {function_name}({arguments})")
            result = await self.tool_manager.execute_tool(function_name, arguments)
            logger.info(f"âœ… å·¥å…·ç»“æœ: {result}")
            return result
        except Exception as e:
            error_msg = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}", exc_info=True)
            return error_msg

    def _create_tool_functions(self):
        """åˆ›å»ºå·¥å…·å‡½æ•°åˆ—è¡¨"""
        from livekit.agents.llm import function_tool

        tools = []
        for tool_name, tool_config in self.tool_manager.tools.items():
            # æå–å·¥å…·æè¿°å’Œå‚æ•°
            description = tool_config.get('description', f'å·¥å…·: {tool_name}')
            parameters = tool_config.get('parameters', {})

            # âœ… ä½¿ç”¨é—­åŒ…æ•è·æ­£ç¡®çš„å˜é‡
            def make_tool_func(name: str):
                async def tool_func(**kwargs):
                    return await self._execute_tool(name, kwargs)

                tool_func.__name__ = name
                tool_func.__doc__ = description

                # æ·»åŠ å‚æ•°ç±»å‹æ³¨è§£
                annotations = {}
                props = parameters.get('properties', {})
                for param_name, param_info in props.items():
                    param_type = param_info.get('type', 'string')
                    type_map = {
                        'string': str,
                        'number': float,
                        'integer': int,
                        'boolean': bool
                    }
                    annotations[param_name] = type_map.get(param_type, str)

                if annotations:
                    tool_func.__annotations__ = annotations

                return tool_func

            tool_func = make_tool_func(tool_name)
            tools.append(function_tool(tool_func))
            logger.info(f"ğŸ“Œ æ³¨å†Œå·¥å…·: {tool_name} - {description}")

        return tools

    async def process_llm_response(self, chat_context: ChatContext):
        """å¤„ç† LLM å“åº”å¹¶æ’­æ”¾ (æ”¯æŒå·¥å…·è°ƒç”¨)"""
        tts_stream = self.tts.stream()
        full_response = ""

        try:
            # æ·»åŠ ç³»ç»Ÿæç¤º (åªæ·»åŠ ä¸€æ¬¡)
            messages = [item for item in chat_context.items if hasattr(item, 'role')]
            has_system_message = any(msg.role == "system" for msg in messages)

            if not has_system_message:
                chat_context.add_message(
                    role="system",
                    content=(
                        "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ã€‚"
                        "å½“ç”¨æˆ·è¯¢é—®å¤©æ°”æ—¶ï¼Œä½¿ç”¨ get_weather å·¥å…·è·å–å®æ—¶ä¿¡æ¯ã€‚"
                        "ç”¨ç®€æ´å‹å¥½çš„è¯­æ°”å›ç­”ï¼Œç›´æ¥è¯´å‡ºæ¸©åº¦å’Œå¤©æ°”çŠ¶å†µï¼Œä¸è¦è¯´'æ ¹æ®æŸ¥è¯¢ç»“æœ'ä¹‹ç±»çš„è¯ã€‚"
                    )
                )

            # åˆ›å»ºå·¥å…·ä¸Šä¸‹æ–‡
            from livekit.agents.llm import ToolContext
            tools = self._create_tool_functions()
            tool_ctx = ToolContext(tools) if tools else None

            # âœ… è°ƒç”¨ LLM (å…¼å®¹é˜¿é‡Œäº‘æ’ä»¶)
            try:
                # æ–¹å¼ 1: ä½¿ç”¨ tool_ctx (æ ‡å‡†æ–¹å¼)
                llm_stream = self.llm.chat(
                    chat_ctx=chat_context,
                    tool_ctx=tool_ctx
                )
                logger.debug("âœ… ä½¿ç”¨ tool_ctx æ¨¡å¼")

            except TypeError as e:
                # æ–¹å¼ 2: ä¸ä½¿ç”¨å·¥å…· (é™çº§)
                logger.warning(f"âš ï¸ LLM ä¸æ”¯æŒ tool_ctxï¼Œé™çº§åˆ°æ™®é€šæ¨¡å¼: {e}")
                llm_stream = self.llm.chat(chat_ctx=chat_context)

            # å¤„ç†æµå¼å“åº”
            pending_tool_calls = []

            async for chunk in llm_stream:
                # âœ… æ£€æŸ¥ LiveKit åŸç”Ÿçš„å·¥å…·è°ƒç”¨æ ¼å¼
                if isinstance(chunk, FunctionCall):
                    logger.info(f"ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {chunk.name}")
                    pending_tool_calls.append(chunk)
                    continue

                # âœ… æ£€æŸ¥ OpenAI é£æ ¼çš„å·¥å…·è°ƒç”¨
                if hasattr(chunk, 'choices') and chunk.choices:
                    choice = chunk.choices[0]

                    # å·¥å…·è°ƒç”¨è¯·æ±‚
                    if hasattr(choice, 'message') and hasattr(choice.message,
                                                              'tool_calls') and choice.message.tool_calls:
                        for tool_call in choice.message.tool_calls:
                            pending_tool_calls.append(tool_call)
                        continue

                    # æ–‡æœ¬å†…å®¹
                    if hasattr(choice, 'delta') and choice.delta and choice.delta.content:
                        content = choice.delta.content
                        tts_stream.push_text(content)
                        full_response += content

                # âœ… ç›´æ¥çš„ delta æ ¼å¼
                elif hasattr(chunk, 'delta') and chunk.delta and hasattr(chunk.delta,
                                                                         'content') and chunk.delta.content:
                    content = chunk.delta.content
                    tts_stream.push_text(content)
                    full_response += content

                # âœ… ç®€å•çš„ content æ ¼å¼
                elif hasattr(chunk, 'content') and chunk.content:
                    content = chunk.content
                    tts_stream.push_text(content)
                    full_response += content

            # âœ… æ‰§è¡Œå¾…å¤„ç†çš„å·¥å…·è°ƒç”¨
            if pending_tool_calls:
                logger.info(f"ğŸ“‹ å¤„ç† {len(pending_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")

                for tool_call in pending_tool_calls:
                    # å…¼å®¹å¤šç§æ ¼å¼
                    if isinstance(tool_call, FunctionCall):
                        # LiveKit åŸç”Ÿæ ¼å¼
                        function_name = tool_call.name
                        arguments = json.loads(tool_call.arguments) if isinstance(tool_call.arguments,
                                                                                  str) else tool_call.arguments
                        call_id = tool_call.call_id
                    else:
                        # OpenAI æ ¼å¼
                        function_name = tool_call.function.name
                        arguments = json.loads(tool_call.function.arguments)
                        call_id = tool_call.id

                    logger.info(f"ğŸ”§ æ‰§è¡Œ: {function_name}({arguments})")

                    # æ·»åŠ å‡½æ•°è°ƒç”¨åˆ°ä¸Šä¸‹æ–‡
                    func_call = FunctionCall(
                        call_id=call_id,
                        name=function_name,
                        arguments=json.dumps(arguments, ensure_ascii=False)
                    )
                    chat_context.items.append(func_call)

                    # æ‰§è¡Œå·¥å…·
                    tool_result = await self._execute_tool(function_name, arguments)

                    # æ·»åŠ å‡½æ•°è¾“å‡ºåˆ°ä¸Šä¸‹æ–‡
                    func_output = FunctionCallOutput(
                        call_id=call_id,
                        name=function_name,
                        output=tool_result,
                        is_error=False
                    )
                    chat_context.items.append(func_output)

                # âœ… é€’å½’è°ƒç”¨ï¼Œè®© AI æ ¹æ®å·¥å…·ç»“æœç”Ÿæˆè‡ªç„¶è¯­è¨€å›ç­”
                logger.info("ğŸ”„ æ ¹æ®å·¥å…·ç»“æœç”Ÿæˆå›ç­”...")
                await self.process_llm_response(chat_context)
                return

            # ä¿å­˜å¹¶æ’­æ”¾åŠ©æ‰‹å›å¤
            if full_response:
                tts_stream.flush()
                chat_context.add_message(
                    role="assistant",
                    content=full_response
                )
                logger.info(f"ğŸ¤– AI: {full_response}")

                # æ’­æ”¾éŸ³é¢‘
                async for audio_chunk in tts_stream:
                    if hasattr(audio_chunk, 'frame'):
                        await self.audio_source.capture_frame(audio_chunk.frame)
                    else:
                        await self.audio_source.capture_frame(audio_chunk)

            await tts_stream.aclose()

        except Exception as e:
            logger.error(f"âŒ LLM/TTS é”™è¯¯: {e}", exc_info=True)
            # å‘é€é”™è¯¯æç¤º
            try:
                error_text = "æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œè¯·ç¨åå†è¯•ã€‚"
                tts_stream.push_text(error_text)
                tts_stream.flush()
                async for audio_chunk in tts_stream:
                    if hasattr(audio_chunk, 'frame'):
                        await self.audio_source.capture_frame(audio_chunk.frame)
                    else:
                        await self.audio_source.capture_frame(audio_chunk)
                await tts_stream.aclose()
            except:
                pass

    async def connect_to_room(self):
        """è¿æ¥åˆ° LiveKit æˆ¿é—´"""
        token = (
            api.AccessToken(settings.LIVEKIT_API_KEY, settings.LIVEKIT_API_SECRET)
            .with_identity(settings.AGENT_IDENTITY)
            .with_name("AI Assistant")
            .with_grants(api.VideoGrants(
                room_join=True,
                room=settings.ROOM_NAME,
                can_publish=True,
                can_publish_data=True,
                agent=True,
            ))
        ).to_jwt()

        self.room = rtc.Room()

        try:
            await self.room.connect(settings.LIVEKIT_URL, token)
            logger.info(f"âœ… å·²è¿æ¥åˆ°æˆ¿é—´: {settings.ROOM_NAME}")
        except Exception as e:
            raise LiveKitConnectionError(f"è¿æ¥å¤±è´¥: {e}")

        # åˆ›å»ºéŸ³é¢‘è½¨é“
        self.audio_source = rtc.AudioSource(
            self.tts.sample_rate,
            self.tts.num_channels
        )
        track = rtc.LocalAudioTrack.create_audio_track(
            "ai-voice",
            self.audio_source
        )
        await self.room.local_participant.publish_track(track)
        logger.info("ğŸ¤ AI è¯­éŸ³è½¨é“å·²å‘å¸ƒ")

    async def process_participant_audio(self, participant: rtc.Participant):
        """å¤„ç†å‚ä¸è€…éŸ³é¢‘"""
        if participant.identity == settings.AGENT_IDENTITY:
            return

        # æŸ¥æ‰¾éº¦å…‹é£è½¨é“
        audio_stream = None
        for pub in participant.track_publications.values():
            if (pub.track and
                    pub.kind == rtc.TrackKind.KIND_AUDIO and
                    pub.source == rtc.TrackSource.SOURCE_MICROPHONE):
                audio_stream = rtc.AudioStream(pub.track)
                break

        if not audio_stream:
            logger.warning(f"æœªæ‰¾åˆ° {participant.identity} çš„éº¦å…‹é£")
            return

        logger.info(f"ğŸ§ å¼€å§‹å¤„ç†: {participant.identity}")

        stt_stream = self.stt.stream()
        chat_context = ChatContext()

        async def feed_stt():
            """éŸ³é¢‘é‡é‡‡æ ·å¹¶å‘é€åˆ° STT"""
            resampler = rtc.AudioResampler(
                input_rate=48000,
                output_rate=16000,
                num_channels=1,
                quality=rtc.AudioResamplerQuality.QUICK
            )

            async for frame_event in audio_stream:
                frame = frame_event.frame
                for resampled_frame in resampler.push(frame):
                    stt_stream.push_frame(resampled_frame)

            for resampled_frame in resampler.flush():
                stt_stream.push_frame(resampled_frame)

            await stt_stream.aclose()

        async def handle_stt():
            """å¤„ç† STT ç»“æœ"""
            async for event in stt_stream:
                if (event.type == SpeechEventType.FINAL_TRANSCRIPT and
                        event.alternatives):
                    user_text = event.alternatives[0].text.strip()
                    if not user_text:
                        continue

                    logger.info(f"ğŸ’¬ ç”¨æˆ·: {user_text}")
                    chat_context.add_message(
                        role="user",
                        content=user_text
                    )

                    # å¼‚æ­¥å¤„ç† LLM + TTS
                    asyncio.create_task(self.process_llm_response(chat_context))

        await asyncio.gather(
            feed_stt(),
            handle_stt(),
            return_exceptions=True
        )

    async def start(self):
        """å¯åŠ¨åŠ©æ‰‹"""
        try:
            await self.initialize()
            await self.connect_to_room()

            @self.room.on("track_subscribed")
            def on_track_subscribed(
                    track: rtc.Track,
                    publication: rtc.RemoteTrackPublication,
                    participant: rtc.RemoteParticipant
            ):
                if (publication.kind == rtc.TrackKind.KIND_AUDIO and
                        publication.source == rtc.TrackSource.SOURCE_MICROPHONE):
                    logger.info(f"ğŸ¤ æ£€æµ‹åˆ°éº¦å…‹é£: {participant.identity}")
                    asyncio.create_task(self.process_participant_audio(participant))

            @self.room.on("participant_connected")
            def on_participant_connected(participant: rtc.RemoteParticipant):
                logger.info(f"ğŸ‘¤ ç”¨æˆ·åŠ å…¥: {participant.identity}")

            logger.info("âœ¨ AI Agent å°±ç»ª")
            await asyncio.Event().wait()

        except Exception as e:
            logger.error(f"é”™è¯¯: {e}", exc_info=True)
        finally:
            await self.cleanup()

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.http_session:
            await self.http_session.close()
        if self.room:
            await self.room.disconnect()
        logger.info("ğŸšª AI åŠ©æ‰‹å·²å…³é—­")